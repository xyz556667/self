---
title: "AS10-2：研究是否有做臨床實驗"
author: "卓雍然 D994010001"
date: "`r Sys.time()`"
output: 
  html_document:
    highlight: pygments
    theme: flatly
    css: style.css
---

<br><hr>

```{r}
packages = c(
  "dplyr","ggplot2","caTools","tm","SnowballC","ROCR","rpart",
  "rpart.plot","randomForest")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
```

```{r warning=F, message=F, cache=F, error=F}
rm(list=ls(all=TRUE))
Sys.setlocale("LC_ALL","C")
options(digits=5, scipen=10)

library(dplyr)
library(tm)
library(SnowballC)
library(ROCR)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
```
<br>

### Problem 1 - Data Exploration

```{r}
D = read.csv("data/clinical_trial.csv", stringsAsFactors = F)
```

##### 1.1 The longest abstract
【P1.1】_How many characters are there in the longest abstract?_
```{r}
nchar(D$abstract) %>% max
```

##### 1.2 No abstract
【P1.2】_How many search results provided no abstract?_
```{r}
sum(nchar(D$abstract) == 0)
```

##### 1.3 Shortest title
【P1.3】_What is the shortest title?_
```{r}
D$title[ which.min(nchar(D$title)) ]
```
<br><hr>

### Problem 2 - Preparing the Corpus

Because we have both title and abstract information for trials, we need to build two corpora instead of one. Name them `corpT` and `corpA`.

##### 2.1 Courps & DTM
```{r}
library(tm)
library(SnowballC)

# Corpus & DTM for Title
corpT = Corpus(VectorSource(D$title))
corpT = tm_map(corpT,  content_transformer(tolower))
corpT = tm_map(corpT, removePunctuation)
corpT = tm_map(corpT, removeWords, stopwords("english"))
corpT = tm_map(corpT, stemDocument)
dtmT = DocumentTermMatrix(corpT); dtmT
dtmT = removeSparseTerms(dtmT, 0.95); dtmT 
dtmT = as.data.frame(as.matrix(dtmT))
```
【P2.1a】_How many terms remain in `dtmT` after removing sparse terms (aka how many columns does it have)?_

+ 31
+

```{r}
corpA = Corpus(VectorSource(D$abstract))
corpA = tm_map(corpA,  content_transformer(tolower))
corpA = tm_map(corpA, removePunctuation)
corpA = tm_map(corpA, removeWords, stopwords("english"))
corpA = tm_map(corpA, stemDocument)
dtmA = DocumentTermMatrix(corpA); dtmA
dtmA = removeSparseTerms(dtmA, 0.95); dtmA 
dtmA = as.data.frame(as.matrix(dtmA))
```
【P2.1b】_How many terms remain in `dtmA` after removing sparse terms?_

+ 335
+

##### 2.2 Abstract is longer than title
【P2.2】_What is the most likely reason why `dtmA` has so many more terms than `dtmT`?_

+ Abstracts tend to have many more words than titles
+

##### 2.3 The most frequent word in abstracts
【P2.3】_What is the most frequent word stem across all the abstracts?_
```{r}
which.max(colSums(dtmA))
```
<br><hr>

### Problem 3 - Building a model

##### 3.1 Make column names
```{r}
colnames(dtmT) = paste0("T", colnames(dtmT))
colnames(dtmA) = paste0("A", colnames(dtmA))
```
【P3.1】_What was the effect of these functions?_

+ Adding the letter T in front of all the title variable names and adding the letter A in front of all the abstract variable names.
+

##### 3.2 Combine DTMs & Add target varible
```{r}
dtm = cbind(dtmT, dtmA)
dtm$trial = D$trial
```

【P3.2】_How many columns are in this combined data frame?_
```{r}
ncol(dtm)
```

##### 3.3 Splitting data
```{r}
library(caTools)
set.seed(144)
spl = sample.split(dtm$trial, 0.7)
train = subset(dtm, spl == TRUE)
test = subset(dtm, spl == FALSE)
```

【P3.3】_What is the accuracy of the baseline model on the training set?_
```{r}
table(train$trial) %>% prop.table
```

##### 3.4 CART Model
```{r}
library(rpart)
library(rpart.plot)
cart = rpart(trial~., train, method="class")
```

【P3.4】_What is the name of the first variable the model split on?_
```{r}
prp(cart)
```

##### 3.5 The predicted probability
【P3.5】_What is the maximum predicted probability for any result?_
```{r}
pred = predict(cart)[,2]
max(pred)
```

##### 3.6 Similarity between testing & training data
【P3.6】_Without running the analysis, how do you expect the maximum predicted probability to differ in the testing set?_

+ The maximum predicted probability will likely be exactly the same in the testing set.
+

##### 3.7 Accuracy Matrices
Use a threshold probability of 0.5 
```{r}
table(train$trial, pred > 0.5)
```

【P3.7a】_What is the training set accuracy of the CART model?_
```{r}
(631+441)/(631+441+131+99)
```

【P3.7b】_What is the training set sensitivity of the CART model?_
```{r}
441/(131+441)
```

【P3.7c】_What is the training set specificity of the CART model?_
```{r}
631/(631+99)
```
<br><hr>

### Problem 4 - Evaluating the model on the testing set

##### 4.1 Test Accuracy
【P4.1】_What is the test accuracy?_
```{r}
pred = predict(cart,test)[,2]
table(test$trial, pred > 0.5) %>% {sum(diag(.)) / sum(.)}
```

##### 4.2 Test AUC
【P4.2】_What is the test AUC?_
```{r}
colAUC(pred, test$trial)
```
<br><hr>

### Problem 5 - Decision Tradeoffs
The research procedure is ...

+ Step 1: use model to select articles of "predict trial = 1"
+ Step 2: manually review the selected articles
+ Step 3: extract data from suitable articles (for further analysis)

##### 5.1 Cost of False Negative
【P5.1】_What is the cost associated with the model in Step 1 making a false negative prediction?_

+ A paper that should have been included in Set A will be missed, affecting the quality of the results of Step 3
+

##### 5.2 Cost of False Posituve
【P5.2】_What is the cost associated with the model in Step 1 making a false positive prediction?_

+ A paper will be mistakenly added to Set A, yielding additional work in Step 2 of the process but not affecting the quality of the results of Step 3.
+

##### 5.3 The Threshold
【P5.3】_Given the costs associated with false positives and false negatives, which of the following is most accurate?_

+ A false negative is more costly than a false positive; the decision maker should use a probability threshold less than 0.5 for the machine learning model. 
+

<br><hr><br><br><br><br>





